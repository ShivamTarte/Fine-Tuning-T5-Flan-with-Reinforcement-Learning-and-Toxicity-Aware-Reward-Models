{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-01-19T15:18:30.860505Z","iopub.status.busy":"2025-01-19T15:18:30.860257Z","iopub.status.idle":"2025-01-19T15:18:48.363959Z","shell.execute_reply":"2025-01-19T15:18:48.363174Z","shell.execute_reply.started":"2025-01-19T15:18:30.860482Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting peft\n","  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n","Collecting huggingface-hub>=0.25.0 (from peft)\n","  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.6.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: huggingface-hub, peft\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.24.7\n","    Uninstalling huggingface-hub-0.24.7:\n","      Successfully uninstalled huggingface-hub-0.24.7\n","Successfully installed huggingface-hub-0.27.1 peft-0.14.0\n","Collecting trl==0.11.4\n","  Downloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.11.4) (2.4.1+cu121)\n","Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.11.4) (4.44.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl==0.11.4) (0.34.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl==0.11.4) (3.2.0)\n","Collecting tyro>=0.5.11 (from trl==0.11.4)\n","  Downloading tyro-0.9.11-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl==0.11.4) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.11.4) (2024.6.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.27.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl==0.11.4) (4.66.5)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.4) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.4) (13.8.1)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.4)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.4) (4.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl==0.11.4) (5.9.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.11.4) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.11.4) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.11.4) (2.1.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.11.4) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.11.4) (0.70.16)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.11.4) (3.10.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.11.4) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl==0.11.4) (2024.8.30)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (2.18.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl==0.11.4) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.11.4) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.11.4) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.11.4) (2024.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl==0.11.4) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.11.4) (1.16.0)\n","Downloading trl-0.11.4-py3-none-any.whl (316 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading tyro-0.9.11-py3-none-any.whl (115 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Installing collected packages: shtab, tyro, trl\n","Successfully installed shtab-1.7.1 trl-0.11.4 tyro-0.9.11\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.27.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.3\n","/kaggle/input/healthcare-dataset/Doctor-HealthCare-100k.csv\n","/kaggle/input/toxic-101/CategorySubcategoryPrompt.csv\n","/kaggle/input/gemma-language-tuning/submission_instructions.txt\n"]}],"source":["# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","!pip install peft\n","# !pip install transformers==4.28.1\n","!pip install trl==0.11.4\n","!pip install evaluate\n","# %pip install git+https://github.com/lvwerra/trl.git@25fa1bd    \n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"reihanenamdari/youtube-toxicity-data\")\n","\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:18:48.365231Z","iopub.status.busy":"2025-01-19T15:18:48.364802Z","iopub.status.idle":"2025-01-19T15:18:48.415754Z","shell.execute_reply":"2025-01-19T15:18:48.414742Z","shell.execute_reply.started":"2025-01-19T15:18:48.365190Z"},"trusted":true},"outputs":[],"source":["if torch.cuda.is_available():\n","    device='cuda'\n","else:\n","    device='cpu'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:18:48.417228Z","iopub.status.busy":"2025-01-19T15:18:48.416887Z","iopub.status.idle":"2025-01-19T15:18:56.742149Z","shell.execute_reply":"2025-01-19T15:18:56.741507Z","shell.execute_reply.started":"2025-01-19T15:18:48.417194Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"153c327593f44be8bd049d4d9d91bc96","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8da7b5b8b8cb43b49ecb40d596936217","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4dd529abf14a441fa07e314c04da4948","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb2db9820b014033b149c4019e5a1e3d","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a71a304a6d364f1ea919b89207413024","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b05d3e4721d54b51896941e83a360fcd","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9646c7045b72452886f7a7d2c5b1664a","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load model directly\n","model_name=\"google/flan-t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\",device_map='auto').to(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:18:56.744145Z","iopub.status.busy":"2025-01-19T15:18:56.743810Z","iopub.status.idle":"2025-01-19T15:18:56.749780Z","shell.execute_reply":"2025-01-19T15:18:56.748986Z","shell.execute_reply.started":"2025-01-19T15:18:56.744125Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["None\n","device count 1\n","name of Device Tesla P100-PCIE-16GB\n"]}],"source":["print(model.device_map)\n","if torch.cuda.is_available():\n","    no_device=torch.cuda.device_count()\n","    print(\"device count\",no_device)\n","    for i in range(no_device):\n","        print(\"name of Device\",torch.cuda.get_device_name(i))"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:18:56.751674Z","iopub.status.busy":"2025-01-19T15:18:56.751319Z","iopub.status.idle":"2025-01-19T15:18:56.861941Z","shell.execute_reply":"2025-01-19T15:18:56.861032Z","shell.execute_reply.started":"2025-01-19T15:18:56.751645Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model on GPU: 282\n"]}],"source":["print(f\"Model on GPU:\", len([(p.is_cuda) for p in model.parameters()]))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:18:56.863190Z","iopub.status.busy":"2025-01-19T15:18:56.862877Z","iopub.status.idle":"2025-01-19T15:18:56.879159Z","shell.execute_reply":"2025-01-19T15:18:56.878372Z","shell.execute_reply.started":"2025-01-19T15:18:56.863159Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["282\n"]}],"source":["# for name, param in model.named_parameters():\n","#     print(f\"Parameter name: {name}\") \n","\n","print(len([i for i,_ in model.named_parameters()]))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:18:56.880193Z","iopub.status.busy":"2025-01-19T15:18:56.879943Z","iopub.status.idle":"2025-01-19T15:18:57.101306Z","shell.execute_reply":"2025-01-19T15:18:57.100609Z","shell.execute_reply.started":"2025-01-19T15:18:56.880163Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM\n","from peft import LoraModel, LoraConfig,get_peft_model\n","\n","config = LoraConfig(\n","    task_type=\"SEQ_2_SEQ_LM\",\n","    r=6,\n","    lora_alpha=32,\n","    target_modules=[\"q\", \"v\"],\n","    lora_dropout=0.01,\n",")\n","\n","lora_model=get_peft_model(model,config,'default')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:18:57.102287Z","iopub.status.busy":"2025-01-19T15:18:57.102069Z","iopub.status.idle":"2025-01-19T15:19:06.344701Z","shell.execute_reply":"2025-01-19T15:19:06.343747Z","shell.execute_reply.started":"2025-01-19T15:18:57.102269Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Output: M' collègue est a l'assaint.\n"]}],"source":["inputs = tokenizer.encode(\"translate english into french: My colleague is bitch\",return_tensors='pt').to(device)  # Batch size 1\n","outputs = model.generate(inputs)\n","print('Output:',tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:06.346005Z","iopub.status.busy":"2025-01-19T15:19:06.345432Z","iopub.status.idle":"2025-01-19T15:19:11.200781Z","shell.execute_reply":"2025-01-19T15:19:11.200051Z","shell.execute_reply.started":"2025-01-19T15:19:06.345980Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85bd8dc8f0ef4755901ac18df907da2a","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48cee26d66194668b928a118f7e84961","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eace1ee77dd845d9b1f67f5a40638b86","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e71411a114e45efb46717a9c6ddb13d","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a382019a0f744c14843becc163396695","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9c6148528fb4c2dbac0ff9ff9ef556d","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","\n","tokenizer_tox= RobertaTokenizer.from_pretrained('s-nlp/roberta_toxicity_classifier')\n","model_tox = RobertaForSequenceClassification.from_pretrained('s-nlp/roberta_toxicity_classifier').to(device)\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:11.201735Z","iopub.status.busy":"2025-01-19T15:19:11.201511Z","iopub.status.idle":"2025-01-19T15:19:11.340291Z","shell.execute_reply":"2025-01-19T15:19:11.339381Z","shell.execute_reply.started":"2025-01-19T15:19:11.201715Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities: tensor(4.8142, device='cuda:0', grad_fn=<SelectBackward0>)\n"]}],"source":["\n","from torch.nn.functional import softmax\n","\n","def reward_model(prompt):\n","    \n","    batch = tokenizer_tox.encode(prompt, return_tensors=\"pt\").to(device)\n","    \n","    output = model_tox(batch)\n","    \n","    output=output.logits\n","    \n","    # # Apply softmax to convert logits into probabilities\n","    # probabilities = softmax(output, dim=1).tolist()[0]\n","    \n","    # if probabilities[1]>0.5:\n","    #     probabilities=1\n","    # else:\n","    #     probabilities=0\n","\n","    return output[0][0]\n","    \n","\n","\n","print(\"Probabilities:\", reward_model('You are beautiful'))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:11.343286Z","iopub.status.busy":"2025-01-19T15:19:11.343050Z","iopub.status.idle":"2025-01-19T15:19:11.368504Z","shell.execute_reply":"2025-01-19T15:19:11.367844Z","shell.execute_reply.started":"2025-01-19T15:19:11.343266Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Prompt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Write a story about a rumor that spirals out o...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Describe the emotional impact of being the tar...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Create a scenario where misinformation on soci...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Write about a character who fabricates stories...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Describe the spread of a conspiracy theory and...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>173</th>\n","      <td>Create a scene of public shaming related to so...</td>\n","    </tr>\n","    <tr>\n","      <th>174</th>\n","      <td>Write about the impact of revenge porn on the ...</td>\n","    </tr>\n","    <tr>\n","      <th>175</th>\n","      <td>Describe a situation where someone uses racial...</td>\n","    </tr>\n","    <tr>\n","      <th>176</th>\n","      <td>Create a scenario where someone spreads false ...</td>\n","    </tr>\n","    <tr>\n","      <th>177</th>\n","      <td>Write about the consequences of cyberstalking ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>178 rows × 1 columns</p>\n","</div>"],"text/plain":["                                                Prompt\n","0    Write a story about a rumor that spirals out o...\n","1    Describe the emotional impact of being the tar...\n","2    Create a scenario where misinformation on soci...\n","3    Write about a character who fabricates stories...\n","4    Describe the spread of a conspiracy theory and...\n","..                                                 ...\n","173  Create a scene of public shaming related to so...\n","174  Write about the impact of revenge porn on the ...\n","175  Describe a situation where someone uses racial...\n","176  Create a scenario where someone spreads false ...\n","177  Write about the consequences of cyberstalking ...\n","\n","[178 rows x 1 columns]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["\n","\n","df=pd.read_csv('/kaggle/input/toxic-101/CategorySubcategoryPrompt.csv',usecols=[\"Prompt\"])\n","df"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:11.370336Z","iopub.status.busy":"2025-01-19T15:19:11.370126Z","iopub.status.idle":"2025-01-19T15:19:11.373697Z","shell.execute_reply":"2025-01-19T15:19:11.373004Z","shell.execute_reply.started":"2025-01-19T15:19:11.370319Z"},"trusted":true},"outputs":[],"source":["# df.head(5)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:11.374633Z","iopub.status.busy":"2025-01-19T15:19:11.374394Z","iopub.status.idle":"2025-01-19T15:19:11.537664Z","shell.execute_reply":"2025-01-19T15:19:11.536907Z","shell.execute_reply.started":"2025-01-19T15:19:11.374613Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                Prompt  \\\n","0    Write a story about a rumor that spirals out o...   \n","1    Describe the emotional impact of being the tar...   \n","2    Create a scenario where misinformation on soci...   \n","3    Write about a character who fabricates stories...   \n","4    Describe the spread of a conspiracy theory and...   \n","..                                                 ...   \n","173  Create a scene of public shaming related to so...   \n","174  Write about the impact of revenge porn on the ...   \n","175  Describe a situation where someone uses racial...   \n","176  Create a scenario where someone spreads false ...   \n","177  Write about the consequences of cyberstalking ...   \n","\n","                                             input_ids  \n","0    [8733, 3, 9, 733, 81, 3, 9, 3, 2781, 127, 24, ...  \n","1    [3, 30010, 8, 3973, 1113, 13, 271, 8, 2387, 13...  \n","2    [6357, 3, 9, 8616, 213, 1817, 6391, 30, 569, 7...  \n","3    [8733, 81, 3, 9, 1848, 113, 22717, 15, 7, 1937...  \n","4    [3, 30010, 8, 3060, 13, 3, 9, 25662, 4516, 11,...  \n","..                                                 ...  \n","173  [6357, 3, 9, 3112, 13, 452, 3, 7, 1483, 53, 13...  \n","174  [8733, 81, 8, 1113, 13, 27831, 5569, 29, 30, 8...  \n","175  [3, 30010, 3, 9, 1419, 213, 841, 2284, 3, 52, ...  \n","176  [6357, 3, 9, 8616, 213, 841, 3060, 7, 6136, 29...  \n","177  [8733, 81, 8, 7702, 13, 9738, 8407, 1765, 11, ...  \n","\n","[178 rows x 2 columns]\n"]}],"source":["def tokenize(sample):\n","    text=sample['Prompt']\n","    sample['input_ids']=tokenizer.encode(text,return_tensors='pt')\n","    sample['input_ids']=sample[\"input_ids\"].squeeze().tolist()\n","\n","    return sample\n","\n","df=df.apply(tokenize, axis=1)\n","\n","#df.set_format(type='torch')\n","\n","print(df)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:11.538860Z","iopub.status.busy":"2025-01-19T15:19:11.538494Z","iopub.status.idle":"2025-01-19T15:19:11.542405Z","shell.execute_reply":"2025-01-19T15:19:11.541635Z","shell.execute_reply.started":"2025-01-19T15:19:11.538826Z"},"trusted":true},"outputs":[],"source":["# batch_size=64\n","# dataset = DataLoader(\n","#     df, batch_size=batch_size, shuffle=True\n","# )\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:11.543481Z","iopub.status.busy":"2025-01-19T15:19:11.543195Z","iopub.status.idle":"2025-01-19T15:19:11.558113Z","shell.execute_reply":"2025-01-19T15:19:11.557405Z","shell.execute_reply.started":"2025-01-19T15:19:11.543452Z"},"trusted":true},"outputs":[],"source":["# #for ind,(input1,output1) in enumerate(dataset):\n","# #    print((input1))\n","# for batch_idx, (inputs) in enumerate(dataset):\n","#     print(f\"Batch {batch_idx + 1}:\")\n","#     print(\"Inputs:\", inputs)\n","    \n","#     break  # Stop after the first batch"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:11.559005Z","iopub.status.busy":"2025-01-19T15:19:11.558747Z","iopub.status.idle":"2025-01-19T15:19:13.441090Z","shell.execute_reply":"2025-01-19T15:19:13.440394Z","shell.execute_reply.started":"2025-01-19T15:19:11.558986Z"},"trusted":true},"outputs":[],"source":["from trl import PPOConfig,PPOTrainer,AutoModelForSeq2SeqLMWithValueHead\n","from trl import create_reference_model\n","from trl.core import LengthSampler\n","from tqdm import tqdm\n","tqdm.pandas()\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:13.442378Z","iopub.status.busy":"2025-01-19T15:19:13.441859Z","iopub.status.idle":"2025-01-19T15:19:13.446747Z","shell.execute_reply":"2025-01-19T15:19:13.445802Z","shell.execute_reply.started":"2025-01-19T15:19:13.442356Z"},"trusted":true},"outputs":[],"source":["def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable parameters: {(trainable_model_params / all_model_params) * 100:.2f}%\"\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:13.447916Z","iopub.status.busy":"2025-01-19T15:19:13.447626Z","iopub.status.idle":"2025-01-19T15:19:13.506519Z","shell.execute_reply":"2025-01-19T15:19:13.505869Z","shell.execute_reply.started":"2025-01-19T15:19:13.447885Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ValueHead(\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (summary): Linear(in_features=768, out_features=1, bias=True)\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n",")\n","Trainable_Parameters for ppo:\n","trainable model parameters: 664321\n","all model parameters: 248242177\n","percentage of trainable parameters: 0.27%\n","Trainable_Parameters for Lora :\n","trainable model parameters: 663552\n","all model parameters: 248241408\n","percentage of trainable parameters: 0.27%\n"]}],"source":["ppo_model=AutoModelForSeq2SeqLMWithValueHead.from_pretrained(lora_model,torch_dtype=torch.bfloat16,is_trainable=True)\n","\n","print(ppo_model.v_head)\n","print(f\"Trainable_Parameters for ppo:{print_number_of_trainable_model_parameters(ppo_model)}\")\n","print(f\"Trainable_Parameters for Lora :{print_number_of_trainable_model_parameters(lora_model)}\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:13.507556Z","iopub.status.busy":"2025-01-19T15:19:13.507237Z","iopub.status.idle":"2025-01-19T15:19:13.647048Z","shell.execute_reply":"2025-01-19T15:19:13.646289Z","shell.execute_reply.started":"2025-01-19T15:19:13.507518Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Trainable_Parameters for Ref_model :\n","trainable model parameters: 0\n","all model parameters: 248242177\n","percentage of trainable parameters: 0.00%\n"]}],"source":["ref_model=create_reference_model(ppo_model).to(device)\n","print(f\"Trainable_Parameters for Ref_model :{print_number_of_trainable_model_parameters(ref_model)}\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:13.648259Z","iopub.status.busy":"2025-01-19T15:19:13.647947Z","iopub.status.idle":"2025-01-19T15:19:13.653369Z","shell.execute_reply":"2025-01-19T15:19:13.652553Z","shell.execute_reply.started":"2025-01-19T15:19:13.648225Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n","  warnings.warn(\n"]}],"source":["learning_rate=1.41e-5\n","max_ppo_epochs=1\n","mini_batch_size=4\n","batch_size=16\n","model_name=\"google/flan-t5-base\"\n","\n","# model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n","# ref_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n","\n","config = PPOConfig(\n","    model_name=model_name,\n","    learning_rate=learning_rate,\n","    ppo_epochs=max_ppo_epochs,\n","    mini_batch_size=mini_batch_size,\n","    batch_size=batch_size\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:13.654303Z","iopub.status.busy":"2025-01-19T15:19:13.654097Z","iopub.status.idle":"2025-01-19T15:19:13.680201Z","shell.execute_reply":"2025-01-19T15:19:13.679600Z","shell.execute_reply.started":"2025-01-19T15:19:13.654283Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","\n","dataset = Dataset.from_pandas(df)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:13.681344Z","iopub.status.busy":"2025-01-19T15:19:13.681042Z","iopub.status.idle":"2025-01-19T15:19:13.685564Z","shell.execute_reply":"2025-01-19T15:19:13.684840Z","shell.execute_reply.started":"2025-01-19T15:19:13.681300Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['Prompt', 'input_ids'],\n","    num_rows: 178\n","})\n"]}],"source":["print(dataset)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:13.686599Z","iopub.status.busy":"2025-01-19T15:19:13.686281Z","iopub.status.idle":"2025-01-19T15:19:13.705599Z","shell.execute_reply":"2025-01-19T15:19:13.704968Z","shell.execute_reply.started":"2025-01-19T15:19:13.686572Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Prompt': \"Write a story about a rumor that spirals out of control and ruins someone's life.\", 'input_ids': [8733, 3, 9, 733, 81, 3, 9, 3, 2781, 127, 24, 17723, 7, 91, 13, 610, 11, 3, 23162, 841, 31, 7, 280, 5, 1]}\n"]}],"source":["from torch.utils.data import DataLoader\n","data1 = DataLoader(dataset, batch_size=16, shuffle=True) \n","print(data1.dataset[0])"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:13.706627Z","iopub.status.busy":"2025-01-19T15:19:13.706350Z","iopub.status.idle":"2025-01-19T15:19:13.724657Z","shell.execute_reply":"2025-01-19T15:19:13.723907Z","shell.execute_reply.started":"2025-01-19T15:19:13.706595Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n","Collator output: {'key1': ['value1'], 'key2': ['value2'], 'key3': ['value3']}\n"]}],"source":["def collator(data):\n","    return dict((key, [d[key] for d in data]) for key in data[0])\n","\n","test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n","print(f'Collator input: {test_data}')\n","print(f'Collator output: {collator(test_data)}')"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:13.725525Z","iopub.status.busy":"2025-01-19T15:19:13.725300Z","iopub.status.idle":"2025-01-19T15:19:14.724241Z","shell.execute_reply":"2025-01-19T15:19:14.723328Z","shell.execute_reply.started":"2025-01-19T15:19:13.725494Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n","  warnings.warn(\n"]}],"source":["ppo_trainer = PPOTrainer(config=config, \n","                         model=ppo_model, \n","                         ref_model=ref_model, \n","                         tokenizer=tokenizer,\n","                         # reward_model='s-nlp/roberta_toxicity_classifier',\n","                         dataset=dataset, \n","                         data_collator=collator\n","                        )"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:14.725399Z","iopub.status.busy":"2025-01-19T15:19:14.725082Z","iopub.status.idle":"2025-01-19T15:19:14.729083Z","shell.execute_reply":"2025-01-19T15:19:14.728135Z","shell.execute_reply.started":"2025-01-19T15:19:14.725367Z"},"trusted":true},"outputs":[],"source":["# print(collator(dataset))"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:19:14.730058Z","iopub.status.busy":"2025-01-19T15:19:14.729820Z","iopub.status.idle":"2025-01-19T15:22:49.549989Z","shell.execute_reply":"2025-01-19T15:22:49.549021Z","shell.execute_reply.started":"2025-01-19T15:19:14.730039Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/11 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["16\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 1/11 [00:21<03:34, 21.47s/it]"]},{"name":"stdout","output_type":"stream","text":["objective/kl: 0.0\n","ppo/returns/mean: 1.2562501430511475\n","ppo/policy/advantages_mean: 0.2130695879459381\n","----------------------------------------------------------------------------------------------------\n","16\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 2/11 [00:42<03:12, 21.37s/it]"]},{"name":"stdout","output_type":"stream","text":["objective/kl: -0.006941064260900021\n","ppo/returns/mean: 1.7181180715560913\n","ppo/policy/advantages_mean: 0.3347399830818176\n","----------------------------------------------------------------------------------------------------\n","16\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 3/11 [00:52<02:07, 15.88s/it]"]},{"name":"stdout","output_type":"stream","text":["objective/kl: 0.007598924450576305\n","ppo/returns/mean: 2.514390230178833\n","ppo/policy/advantages_mean: 0.30532732605934143\n","----------------------------------------------------------------------------------------------------\n","16\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▋      | 4/11 [01:10<01:58, 16.89s/it]"]},{"name":"stdout","output_type":"stream","text":["objective/kl: -0.04296738654375076\n","ppo/returns/mean: 1.8698787689208984\n","ppo/policy/advantages_mean: 0.40402811765670776\n","----------------------------------------------------------------------------------------------------\n","16\n"]},{"name":"stderr","output_type":"stream","text":[" 45%|████▌     | 5/11 [01:27<01:41, 16.97s/it]"]},{"name":"stdout","output_type":"stream","text":["objective/kl: -0.06909316033124924\n","ppo/returns/mean: 1.3106977939605713\n","ppo/policy/advantages_mean: -0.007787749171257019\n","----------------------------------------------------------------------------------------------------\n","16\n"]},{"name":"stderr","output_type":"stream","text":[" 55%|█████▍    | 6/11 [01:52<01:38, 19.66s/it]"]},{"name":"stdout","output_type":"stream","text":["objective/kl: -0.0012985402718186378\n","ppo/returns/mean: 1.5020713806152344\n","ppo/policy/advantages_mean: 0.44280508160591125\n","----------------------------------------------------------------------------------------------------\n","16\n"]},{"name":"stderr","output_type":"stream","text":[" 64%|██████▎   | 7/11 [02:11<01:17, 19.29s/it]"]},{"name":"stdout","output_type":"stream","text":["objective/kl: 0.0036070002242922783\n","ppo/returns/mean: 1.4015758037567139\n","ppo/policy/advantages_mean: 0.05513359606266022\n","----------------------------------------------------------------------------------------------------\n","16\n"]},{"name":"stderr","output_type":"stream","text":[" 73%|███████▎  | 8/11 [02:28<00:56, 18.83s/it]"]},{"name":"stdout","output_type":"stream","text":["objective/kl: -0.04906480386853218\n","ppo/returns/mean: 1.7508280277252197\n","ppo/policy/advantages_mean: 0.2287767231464386\n","----------------------------------------------------------------------------------------------------\n","16\n"]},{"name":"stderr","output_type":"stream","text":[" 82%|████████▏ | 9/11 [02:55<00:42, 21.12s/it]"]},{"name":"stdout","output_type":"stream","text":["objective/kl: -0.1523464024066925\n","ppo/returns/mean: 1.1018491983413696\n","ppo/policy/advantages_mean: 0.017886798828840256\n","----------------------------------------------------------------------------------------------------\n","16\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 10/11 [03:20<00:22, 22.29s/it]"]},{"name":"stdout","output_type":"stream","text":["objective/kl: -0.10894279181957245\n","ppo/returns/mean: 1.1649577617645264\n","ppo/policy/advantages_mean: 0.009528182446956635\n","----------------------------------------------------------------------------------------------------\n","16\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 11/11 [03:34<00:00, 19.53s/it]"]},{"name":"stdout","output_type":"stream","text":["objective/kl: 0.057524118572473526\n","ppo/returns/mean: 1.7009756565093994\n","ppo/policy/advantages_mean: 0.11451010406017303\n","----------------------------------------------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["output_min_length = 100\n","output_max_length = 400\n","output_length_sampler = LengthSampler(output_min_length, output_max_length)\n","\n","generation_kwargs = {\n","    \"min_length\": 5,\n","    \"top_k\": 0.0,\n","    \"top_p\": 1.0,\n","    \"do_sample\": True\n","}\n","\n","\n","for step,batch in enumerate(tqdm(ppo_trainer.dataloader)):\n","\n","    prompt = batch[\"input_ids\"]\n","    prompt_tensors = torch.nn.utils.rnn.pad_sequence(\n","        [torch.tensor(p, dtype=torch.long) for p in prompt],  # Convert each list to a tensor\n","        batch_first=True,  # Pad sequences to have consistent lengths\n","        padding_value=0  # Use 0 (or tokenizer.pad_token_id if defined)\n","    ).to(device)\n","    \n","    \n","    response_tensors=[]\n","    \n","    for prompt_tensor in prompt_tensors:\n","        max_new_tokens=output_length_sampler()\n","        generation_kwargs['max_new_tokens']=max_new_tokens\n","        response = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n","    \n","        response_tensors.append(response.squeeze()[-max_new_tokens:])\n","    \n","    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n","    \n","    \n","    query_response_pairs = [q + r for q, r in zip(data1.dataset[step][\"Prompt\"], batch[\"response\"])] \n","   \n","    # inputs = tokenizer(query_response_pairs, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","    reward_tensors=[]\n","    with torch.no_grad():\n","        for i in query_response_pairs:\n","            logits = reward_model(i)\n","            reward_tensors.append(logits)\n","    \n","    # reward_tensors=[torch.tensor(logit) for logit in logits]\n","    print(len(reward_tensors))\n","    # print('Response:',len(response_tensors))\n","    prompt_tensor_list = torch.split(prompt_tensors, 1, dim=0) \n","\n","# Remove the extra dimension added by torch.split\n","    prompt_tensor_list = [tensor.squeeze(0) for tensor in prompt_tensor_list] \n","    \n","    stats = ppo_trainer.step(prompt_tensor_list, response_tensors, reward_tensors)\n","    \n","    ppo_trainer.log_stats(stats, batch, reward_tensors)\n","    \n","    print(f'objective/kl: {stats[\"objective/kl\"]}')\n","    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n","    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n","    print('-' * 100)\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:22:49.551499Z","iopub.status.busy":"2025-01-19T15:22:49.551175Z","iopub.status.idle":"2025-01-19T15:22:49.555017Z","shell.execute_reply":"2025-01-19T15:22:49.554247Z","shell.execute_reply.started":"2025-01-19T15:22:49.551467Z"},"trusted":true},"outputs":[],"source":["# from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n","# from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n","\n","# tox_model='s-nlp/roberta_toxicity_classifier'\n","# from trl import create_reference_model\n","# import evaluate\n","\n","# toxicity_evaluator = evaluate.load(\"toxicity\", \n","#                                     tox_model,\n","#                                     module_type=\"measurement\",\n","#                                     toxic_label=\"toxic\")"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:22:49.556119Z","iopub.status.busy":"2025-01-19T15:22:49.555865Z","iopub.status.idle":"2025-01-19T15:22:49.586639Z","shell.execute_reply":"2025-01-19T15:22:49.585912Z","shell.execute_reply.started":"2025-01-19T15:22:49.556099Z"},"trusted":true},"outputs":[],"source":["# non_toxic_text = \"#Person 1# tells Tommy that he is good person.\"\n","# toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid and toxic.\"\n","# toxicity_score = toxicity_evaluator.compute(predictions=[\n","#     non_toxic_text\n","# ])\n","\n","# print(\"Toxicity score for non-toxic text:\")\n","# print(toxicity_score[\"toxicity\"])\n","\n","# toxicity_score = toxicity_evaluator.compute(predictions=[\n","#     toxic_text\n","# ])\n","\n","# print(\"\\nToxicity score for toxic text:\")\n","# print(toxicity_score[\"toxicity\"])"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:22:49.587776Z","iopub.status.busy":"2025-01-19T15:22:49.587487Z","iopub.status.idle":"2025-01-19T15:22:49.601854Z","shell.execute_reply":"2025-01-19T15:22:49.600916Z","shell.execute_reply.started":"2025-01-19T15:22:49.587755Z"},"trusted":true},"outputs":[],"source":["# def evaluate_toxicity(model, \n","#                       toxicity_evaluator, \n","#                       tokenizer, \n","#                       dataset, \n","#                       num_samples):\n","    \n","#     \"\"\"\n","#     Preprocess the dataset and split it into train and test parts.\n","\n","#     Parameters:\n","#     - model (trl model): Model to be evaluated.\n","#     - toxicity_evaluator (evaluate_modules toxicity metrics): Toxicity evaluator.\n","#     - tokenizer (transformers tokenizer): Tokenizer to be used.\n","#     - dataset (dataset): Input dataset for the evaluation.\n","#     - num_samples (int): Maximum number of samples for the evaluation.\n","        \n","#     Returns:\n","#     tuple: A tuple containing two numpy.float64 values:\n","#     - mean (numpy.float64): Mean of the samples toxicity.\n","#     - std (numpy.float64): Standard deviation of the samples toxicity.\n","#     \"\"\"\n","\n","#     max_new_tokens=100\n","\n","#     toxicities = []\n","#     input_texts = []\n","#     for i, sample in tqdm(enumerate(dataset)):\n","#         input_text = sample[\"query\"]\n","\n","#         if i > num_samples:\n","#             break\n","            \n","#         input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n","        \n","#         generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n","#                                              top_k=0.0,\n","#                                              top_p=1.0,\n","#                                              do_sample=True)\n","\n","#         response_token_ids = model.generate(input_ids=input_ids,\n","#                                             generation_config=generation_config)\n","        \n","#         generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n","        \n","#         toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n","\n","#         toxicities.extend(toxicity_score[\"toxicity\"])\n","\n","#     # Compute mean & std using np.\n","#     mean = np.mean(toxicities)\n","#     std = np.std(toxicities)\n","        \n","#     return mean, std"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:22:49.603033Z","iopub.status.busy":"2025-01-19T15:22:49.602759Z","iopub.status.idle":"2025-01-19T15:22:49.617225Z","shell.execute_reply":"2025-01-19T15:22:49.616341Z","shell.execute_reply.started":"2025-01-19T15:22:49.603013Z"},"trusted":true},"outputs":[],"source":["# tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n","\n","# mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model, \n","#                                                                           toxicity_evaluator=toxicity_evaluator, \n","#                                                                           tokenizer=tokenizer, \n","#                                                                           dataset=dataset[\"test\"], \n","#                                                                           num_samples=10)\n","\n","# print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:22:49.621192Z","iopub.status.busy":"2025-01-19T15:22:49.620859Z","iopub.status.idle":"2025-01-19T15:22:49.633216Z","shell.execute_reply":"2025-01-19T15:22:49.632454Z","shell.execute_reply.started":"2025-01-19T15:22:49.621168Z"},"trusted":true},"outputs":[],"source":["# import torch\n","# from torch.nn.utils.rnn import pad_sequence\n","# from tqdm import tqdm\n","# from transformers import AutoTokenizer\n","\n","# # Load tokenizer\n","# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n","\n","# # Function to generate responses\n","# def generate_responses(prompts, ppo_trainer, max_new_tokens=100, generation_kwargs=None, device=\"cuda\"):\n","#     \"\"\"\n","#     Generate responses for a list of prompts using the PPO-trained T5 Flan model.\n","    \n","#     Args:\n","#         prompts (list): List of input prompts.\n","#         ppo_trainer (PPOTrainer): Trained PPO trainer object.\n","#         max_new_tokens (int): Maximum number of new tokens to generate.\n","#         generation_kwargs (dict): Additional generation parameters.\n","#         device (str): Device for computation (\"cuda\" or \"cpu\").\n","    \n","#     Returns:\n","#         List of generated responses.\n","#     \"\"\"\n","#     # Default generation arguments\n","#     if generation_kwargs is None:\n","#         generation_kwargs = {\n","#             \"min_length\": 5,\n","#             \"top_k\": 0.0,\n","#             \"top_p\": 1.0,\n","#             \"do_sample\": True\n","#         }\n","\n","#     # Tokenize and pad prompts\n","#     prompt_tensors = pad_sequence(\n","#         [torch.tensor(tokenizer.encode(p, truncation=True), dtype=torch.long) for p in prompts],\n","#         batch_first=True,\n","#         padding_value=tokenizer.pad_token_id\n","#     ).to(device)\n","\n","#     responses = []\n","\n","#     # Generate responses\n","#     for prompt_tensor in tqdm(prompt_tensors, desc=\"Generating responses\"):\n","#         generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n","#         with torch.no_grad():\n","#             response = ppo_trainer.generate(prompt_tensor.unsqueeze(0), **generation_kwargs)\n","#             response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n","#             responses.append(response_text)\n","\n","#     return responses\n","\n","\n","# # Example prompts\n","# prompts = [\n","#     \"Translate English to French: How are you?\",\n","#     \"Write a story about a heroic knight.\",\n","#     \"Explain the theory of relativity in simple terms.\"\n","# ]\n","\n","# # Generate responses\n","# generated_responses = generate_responses(prompts, ppo_trainer, max_new_tokens=100, device=\"cuda\")\n","\n","# # Print the results\n","# for prompt, response in zip(prompts, generated_responses):\n","#     print(f\"Prompt: {prompt}\")\n","#     print(f\"Response: {response}\")\n","#     print(\"-\" * 80)\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:22:49.634355Z","iopub.status.busy":"2025-01-19T15:22:49.634061Z","iopub.status.idle":"2025-01-19T15:22:49.648898Z","shell.execute_reply":"2025-01-19T15:22:49.648212Z","shell.execute_reply.started":"2025-01-19T15:22:49.634334Z"},"trusted":true},"outputs":[],"source":["text=\"should i humiliate my friend?\""]},{"cell_type":"markdown","metadata":{},"source":["## **Reinforced Model Output** ##"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:22:49.649946Z","iopub.status.busy":"2025-01-19T15:22:49.649666Z","iopub.status.idle":"2025-01-19T15:22:49.890293Z","shell.execute_reply":"2025-01-19T15:22:49.889523Z","shell.execute_reply.started":"2025-01-19T15:22:49.649913Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Output: i should be ashamed of myself\n"]}],"source":["# Ensure input is a single sequence (tensor of shape seq_len)\n","input_ids = tokenizer.encode(text, return_tensors='pt').squeeze(0).to(device)\n","\n","# Generate response\n","with torch.no_grad():\n","    output_ids = ppo_trainer.generate(input_ids)\n","    \n","    # Decode the response\n","    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    print('Output:', output_text)\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Original Model Output** ##"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T15:22:49.891488Z","iopub.status.busy":"2025-01-19T15:22:49.891236Z","iopub.status.idle":"2025-01-19T15:22:50.193182Z","shell.execute_reply":"2025-01-19T15:22:50.192283Z","shell.execute_reply.started":"2025-01-19T15:22:49.891467Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Output: i should i humiliate my friend?\n"]}],"source":["inputs = tokenizer.encode(text,return_tensors='pt').to(device)  # Batch size 1\n","with torch.no_grad():\n","    outputs = model.generate(inputs)\n","    print('Output:',tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":9690815,"sourceId":85416,"sourceType":"competition"},{"datasetId":6339028,"sourceId":10248905,"sourceType":"datasetVersion"},{"datasetId":6356113,"sourceId":10272668,"sourceType":"datasetVersion"}],"dockerImageVersionId":30823,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

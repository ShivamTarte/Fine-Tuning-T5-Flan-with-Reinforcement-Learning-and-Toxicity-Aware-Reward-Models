# Fine-Tuning-T5-Flan-with-Reinforcement-Learning-and-Toxicity-Aware-Reward-Models
In this project, a T5 Flan model was fine-tuned using reinforcement learning (RL) techniques to create a toxicity-aware text generation system. Leveraging the Transformers Reinforcement Learning (TRL) library, the PPO (Proximal Policy Optimization) Trainer was utilized to refine the model's behavior. A Roberta-based toxicity classifier was incorporated into the training loop to guide the model by providing feedback on the generated content's toxicity levels.

To ensure robust training, a custom dataset of toxic prompts was curated. This dataset enabled the model to learn to balance coherent, contextually relevant text generation with a reduced propensity for harmful or toxic outputs. The RL framework utilized the toxicity classifier as a reward model, penalizing toxic generations and encouraging safe, high-quality outputs.

The fine-tuning process was conducted on a GPU, ensuring efficient handling of the computationally intensive training steps. Results demonstrated the effectiveness of the approach in reducing toxicity while maintaining the generative quality of the T5 Flan model. This work highlights the potential of combining advanced reinforcement learning techniques with toxicity-aware reward mechanisms for safer AI text generation.
